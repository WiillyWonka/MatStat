%% -*- coding: utf-8 -*-
\documentclass[12pt,a4paper]{scrartcl} 
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{indentfirst}
\usepackage{misccorr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks,
  pdftitle={The title of your document},
  pdfauthor={Your name},
  allcolors=[RGB]{000 000 000}}

\begin{document}
\begin{titlepage}
  \begin{center}

    Санкт-Петербургский политехнический университет Петра Великого

    \vspace{0.25cm}
    
    Институт прикладной математики и механики
    
    Кафедра «Прикладная математика»
    \vfill

	\vspace{0.25cm}
	    Отчёт\\
	по лабораторной работе №5-8\\
	по дисциплине\\
	«Математическая статистика»

  \bigskip

\end{center}
\vfill

\newlength{\ML}
\settowidth{\ML}{«\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}}}
\hfill\begin{minipage}{0.4\textwidth}
  Выполнил студент\\ В.\,А.~Рыженко\\
\end{minipage}%
\bigskip

\hfill\begin{minipage}{0.4\textwidth}
  Проверил:\\
к.ф.-м.н., доцент\\
Баженов Александр Николаевич\\
\end{minipage}%
\vfill

\begin{center}
  Санкт-Петербург, 2020 г.
\end{center}
\end{titlepage}

\tableofcontents
\listoffigures
\listoftables
\newpage

\section{Постановка задачи}

\begin{enumerate}
\item 
Сгенерировать двумерные выборки размерами 20, 60, 100 для нормального двумерного распределения $N(x,y,0,0,1,1, \rho)$.
Коэффициент корреляции $\rho$ взять равным 0, 0.5, 0.9.
Каждая выборка генерируется 1000 раз и для неё вычисляются: среднее значение, среднее значение квадрата и дисперсия коэффициентов
корреляции Пирсона, Спирмена и квадрантного коэффициента корреляции.
Повторить все вычисления для смеси нормальных распределений:
\begin{equation}\label{eq:smes}
\centering
f(x, y) 0.9N(x, y, 0, 0, 1, 1, 0.9) + 0.1N(x, y, 0, 0, 10, 10, -0.9).
\end{equation}

Изобразить сгенерированные точки на плоскости и нарисовать эллипс
равновероятности.

\item

Найти оценки коэффициентов линейной регрессии $y_{i} = a + bx_{i} + e_{i}$, используя 20 точек на отрезке [-1.8; 2] с равномерным шагом равным 0.2. Ошибку $e_{i}$ считать нормально распределённой с параметрами (0, 1). В качестве эталонной зависимости взять $y_{i} = 2 + 2x_{i} + e_{i}$. При построении оценок коэффициентов использовать два критерия: критерий наименьших квадратов и критерий наименьших модулей. Проделать то же самое для выборки, у которой в значения $y_{1}$ и $y_{20}$ вносятся возмущения 10 и -10.

\item

 Сгенерировать выборку объёмом 100 элементов для нормального распределения N(x,0,1). По сгенерированной выборке оценить параметры $\mu$ и $\sigma$ нормального закона методом максимального правдоподобия. В качестве основной гипотезы $H_{0}$ будем считать, что сгенерированное распределение имеет вид $N(x,\hat{\mu}, \hat{\sigma})$. Проверить основную гипотезу, использу критерий согласия $\chi^{2}$. В качестве уровня значимости взять $\alpha$ = 0.05. Привести таблицу вычислений $\chi^{2}$. 
 \newline
Для анализа чувствительности критерия согласия $\chi^{2}$ сгенерировать выборки равномерного распределения объёмом 10 и 30 элементов.

\item

Для двух выборок размерами 20 и 100 элементов, сгенерированных согласно нормальному закону N(x,0,1), для параметров положения и масштаба построить асимптотически нормальные интервальные оценки на основе точечных оценок метода максимального правдоподобия и классические интервальные оценки на основе статистик $\chi^{2}$ и Стьюдента. В качестве параметра надёжности взять $\gamma$ = 0.95.

\end{enumerate}

\section{Теория}

\subsection{Двумерное нормальное распределение}
	Двумерная случайная величина (X,Y ) называется распределённой нормально (или просто нормальной), если её плотность вероятности определена формулой
	\begin{equation}
	    N(x, y, \bar{x}, \bar{y}, \sigma_{x}, \sigma_{y}, \rho) = 
	    \frac{1}{2\pi\sigma_{x}\sigma_{y}\sqrt{1-\rho^{2}}} \times
	    exp{\begin{Bmatrix}
	        -\frac{1}{2(1-\rho^{2})}
	        \begin{bmatrix}
	        \frac{(x-\bar{x})^{2}}{\sigma_{x}^{2}} - 2\rho\frac{(x-\bar{x})(y-\bar{y})}{\sigma_{x}\sigma_{y}} + \frac{(y-\bar{y})^{2}}{\sigma_{y}^{2}}
            \end{bmatrix}
            \end{Bmatrix}}
	\end{equation}
	Компоненты X,Y двумерной нормальной случайной величины также распределены нормально с математическими ожиданиями $\bar{x}$,$\bar{y}$ и средними квадратическими отклонениями $\sigma_{x},\sigma_{y}$ соответственно.
	Параметр $\rho$ называется коэффициентом корреляции.


	
	\subsection{Корреляционный момент (ковариация) и коэффициент корреляции}
	Корреляционным моментом, иначе ковариацией, двух случайных величин X и Y называется математическое ожидание произведения отклонений этих случайных величин от их математических ожиданий.
	\begin{equation}
	    K = cov(X, Y) = M[(X - \bar{x})(Y - \bar{y})]
	   \label{K}
	\end{equation}
	Коэффициентом корреляции $\rho$ двух случайных величин X и Y называется отношение их корреляционного момента к произведению их средних квадратических отклонений:
	\begin{equation}
	    \rho = \frac{K}{\sigma_{x}\sigma_{y}}
	    \label{ro}
	\end{equation}
	Коэффициент корреляции — это нормированная числовая характеристика, являющаяся мерой близости зависимости между случайными величинами к линейной.

	\subsection{Выборочные коэффициенты корреляции}
	\subsubsection{Выборочный коэффициент корреляции Пирсона}
	Пусть по выборке значений ${x_{i},y_{i}}^{n}_{1}$ двумерной с.в. (X,Y ) требуется оценить коэффициент корреляции $\rho = \frac{cov(X,Y)}{\sqrt{DXDY}}$ . Естественной оценкой для $\rho$ служит его статистический аналог в виде выборочного коэффициента корреляции, предложенного К.Пирсоном, —
	\begin{equation}
	    r = \frac{
	    \frac{1}{n}\sum{(x_{i} - \bar{x})(y_{i}-\bar{y})}
	    }{
	    \sqrt{\frac{1}{n}\sum{(x_{i} - \bar{x})^{2}}\frac{1}{n}\sum{(y_{i} - \bar{y})^{2}}}
	    }=\frac{K}{s_{X}s_{Y}},
	    \label{r}
	\end{equation}
	где $K,s^{2}_{X},s^{2}_{Y}$ — выборочные ковариация и дисперсии с.в. X и Y.


    \subsubsection{Выборочный квадрантный коэффициент корреляции}
    Кроме выборочного коэффициента корреляции Пирсона, существуют и другие оценки степени взаимосвязи между случайными величинами. К ним относится выборочный квадрантный коэффициент корреляции
    \begin{equation}
        r_{Q} = \frac{(n_{1} + n_{3}) - (n_{2} + n_{4})}{n},
        \label{rQ}
    \end{equation}
    где $n_{1}, n_{2},n_{3}$ и $n_{4}$ — количества точке с координатами $(x_{i},y_{i})$, попавшими соответственно в I, II, III и IV квадранты декартовой системы с осями $x' = x-med x, y' = y- med y $ и с центром в точке с координатами (med x,med y).


	  
	\subsubsection{Выборочный коэффициент ранговой корреляции Спирмена}
    На практике нередко требуется оценить степень взаимодействия между качественными признаками изучаемого объекта. Качественным называется признак, который нельзя измерить точно, но который позволяет сравнивать изучаемые объекты между собой и располагать их в порядке убывания или возрастания их качества. Для этого объекты выстраиваются в определённом порядке в соответствии с рассматриваемым признаком. Процесс упорядочения называется ранжированием, и каждому члену упорядоченной последовательности объектов присваивается ранг, или порядковый номер. Например, объекту с наименьшим значением признака присваивается ранг 1, следующему за ним объекту — ранг 2, и т.д. Таким образом, происходит сравнение каждого объекта со всеми объектами изучаемой выборки.
    \newline
    Если объект обладает не одним, а двумя качественными признаками — переменными X и Y , то для исследования их взаимосвязи используют выборочный коэффициент корреляции между двумя последовательностями рангов этих признаков.
    \newline
    Обозначим ранги, соотвествующие значениям переменной X, через u, а ранги, соотвествующие значениям переменной Y, — через v.
    \newline
    Выборочный коэффициент ранговой корреляции Спирмена определяется как выборочный коэффициент корреляции Пирсона между рангами u,v переменных X,Y :
    \begin{equation}
         r_{S} = \frac{
	    \frac{1}{n}\sum{(u_{i} - \bar{u})(v_{i}-\bar{v})}
	    }{
	    \sqrt{\frac{1}{n}\sum{(u_{i} - \bar{u})^{2}}\frac{1}{n}\sum{(v_{i} - \bar{v})^{2}}}
	    },
	    \label{rS}
    \end{equation}
    где $\bar{u} = \bar{v} = \frac{1 + 2 + ... + n}{n} = \frac{n + 1}{2}$ — среднее значение рангов.

	
	\subsection{Эллипсы рассеивания}
	Рассмотрим поверхность распределения, изображающую функцию (1). Она имеет вид холма, вершина которого находится над точкой $(\bar{x},\bar{y})$.
	\newline
    В сечении поверхности распределения плоскостями, параллельными оси $ N(x, y, \bar{x}, \bar{y}, \sigma_{x}, \sigma_{y}, \rho)$, получаются кривые, подобные нормальным кривым распределения. В сечении поверхности распределения плоскостями, параллельными плоскости xOy, получаются эллипсы. Напишем уравнение проекции такого эллипса на плоскость xOy: 
    \begin{equation}
        \frac{(x-\bar{x})^{2}}{\sigma_{x}^{2}} - 
        2\rho\frac{(x-\bar{x})(y-\bar{y})}{\sigma_{x}\sigma_{y}}+
        \frac{(y-\bar{y})^{2}}{\sigma_{y}^{2}} = const
        \label{ellipse}
    \end{equation}
    Уравнение эллипса (8) можно проанализировать обычными методами аналитической геометрии. Применяя их, убеждаемся, что центр эллипса (8) находится в точке с координатами $(\bar{x},\bar{y})$; что касается направления осей симметрии эллипса, то они составляют с осью Ox углы, определяемые уравнением
    \begin{equation}
        \tg(2\alpha) = \frac{2\rho\sigma_{x}\sigma_{y}}{\sigma_{x}^{2} - \sigma_{y}^{2}}
        \label{angle}
    \end{equation}
    Это уравнение дает два значения углов: $\alpha$ и $\alpha_{1}$, различающиеся на $\frac{\pi}{2}$.
    \newline
    Таким образом, ориентация эллипса (8) относительно координатных осей находится в прямой зависимости от коэффициента корреляции $\rho$ системы (X,Y); если величины не коррелированны (т.е. в данном случае и независимы), то оси симметрии эллипса параллельны координатным осям; в противном случае они составляют с координатными осями некоторый угол.
    \newline
    Пересекая поверхность распределения плоскостями, параллельными плоскости xOy, и проектируя сечения на плоскость xOy мы получим целое семейство подобных и одинаково расположенных эллипсов с общим центром $(\bar{x},\bar{y})$. Во всех точках каждого из таких эллипсов плотность распределения $ N(x, y, \bar{x}, \bar{y}, \sigma_{x}, \sigma_{y}, \rho)$ постоянна. Поэтому такие эллипсы называются эллипсами равной плотности или, короче эллипсами рассеивания. Общие оси всех эллипсов рассеивания называются главными осями рассеивания

\subsection{Простая линейная регрессия}

\subsection{Двумерное нормальное распределение}
    Регрессионную модель описания данных называют простой линейной регрессией, если
	\begin{equation}
	    y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i},  i = 1..n
	    \label{y_i}
	\end{equation}
	где $x_{1},...,x_{n}$ — заданные числа (значения фактора); $y_{1},...,y_{n}$ — наблюдаемые значения отклика; $\epsilon_{1},...,\epsilon_{n}$ — независимые, нормально распределённые N(0,$\sigma$) с нулевым математическим ожиданием и одинаковой (неизвестной) дисперсией случайные величины (ненаблюдаемые); $\beta_{0}$, $\beta_{1}$ — неизвестные параметры, подлежащие оцениванию.
    В модели (\ref{y_i}) отклик y зависит зависит от одного фактора x, и весь разброс экспериментальных точек объясняется только погрешностями наблюдений (результатов измерений) отклика y. Погрешности результатов измерений x в этой модели полагают существенно меньшими погрешностей результатов измерений y, так что ими можно пренебречь [1, с. 507].


	
	\subsubsection{Метод наименьших квадратов}
	При оценивании параметров регрессионной модели используют различные методы. Один из наиболее распрстранённых подходов заключается в следующем: вводится мера (критерий) рассогласования отклика и регрессионной функции, и оценки параметров регрессии определяются так, чтобы сделать это рассогласование наименьшим. Достаточно простые расчётные формулы для оценок получают при выборе критерия в виде суммы квадратов отклонений значений отклика от значений регрессионной функции (сумма квадратов остатков):
	\begin{equation}
	    Q(\beta_{0}, \beta_{1}) = \sum_{i=1}^{n}{\epsilon_{i}^{2}} = 
	    \sum_{i=1}^{n}{(y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}}\rightarrow \min_{\beta_{0}, \beta_{1}}
	    \label{Q_beta}
	\end{equation}
	Задача минимизации квадратичного критерия (\ref{Q_beta}) носит название задачи метода наименьших квадратов (МНК), а оценки $\hat{\beta_{0}}, \hat{\beta_{1}}$ параметров $\beta_{0}, \beta_{1}$, реализующие минимум критерия (\ref{Q_beta}), называют МНК-оценками [1, с. 508]. 

	\subsubsection{Расчётные формулы для МНК-оценок}
	МНК-оценки параметров $\hat{\beta_{0}}, \hat{\beta_{1}}$ находятся из условия обращения функции $Q(\beta_{0}, \beta_{1})$ в минимум. 
	\newline
	Для нахождения МНК-оценок $\hat{\beta_{0}}, \hat{\beta_{1}}$ выпишем необходимые условия экстремума
	\begin{equation}
	   \begin{cases}
         & \frac{\partial Q}{\partial \beta_{0}}  = 
         -2\sum_{i=1}^{n}{(y_{i} - \beta_{0} - \beta_{1}x_{i})} = 0\\ 
         & \frac{\partial Q}{\partial \beta_{1}}  = 
         -2\sum_{i=1}^{n}{(y_{i} - \beta_{0} - \beta_{1}x_{i})x_{i}} = 0 
       \end{cases}
       \label{sys_min}
	\end{equation}
	Далее для упрощения записи сумм будем опускать индекс суммирования. Из системы (\ref{sys_min}) получим:
	\begin{equation}
	   \begin{cases}
         & n\hat{\beta_{0}} + \hat{\beta_{1}}\sum_{}{}{x_{i}} = 
         \sum_{}{}{y_{i}}\\ 
        & \hat{\beta_{0}}\sum_{}{}{x_{i}} + \hat{\beta_{1}}\sum_{}{}{x_{i}^{2}} = \sum_{}{}{x_{i}y_{i}}
       \end{cases}
       \label{sys_2}
	\end{equation}
	Разделим оба уравнения на n:
	\begin{equation}
	   \begin{cases}
         & \hat{\beta_{0}} + \hat{\beta_{1}}(\frac{1}{n}\sum_{}{}{x_{i}}) = 
         \frac{1}{n}\sum_{}{}{y_{i}}\\ 
        & \hat{\beta_{0}}(\frac{1}{n}\sum_{}{}{x_{i}}) + \hat{\beta_{1}}(\frac{1}{n}\sum_{}{}{x_{i}^{2}}) = \frac{1}{n}\sum_{}{}{x_{i}y_{i}}
       \end{cases}
       \label{sys_3}
	\end{equation}
	и, используя известные статистические обозначения для выборочных первых и вторых начальных моментов
	\begin{equation}
	    \bar{x} = \frac{1}{n}\sum_{}{}{x_{i}}, \bar{y} = \frac{1}{n}\sum_{}{}{y_{i}}, \bar{x^{2}} = \frac{1}{n}\sum_{}{}{x_{i}^{2}}, \bar{xy} = \frac{1}{n}\sum_{}{}{x_{i}y_{i}}, 
	\end{equation}
	получим
		\begin{equation}
	   \begin{cases}
         & \hat{\beta_{0}} + \hat{\beta_{1}}\bar{x} = 
         \bar{y}\\ 
        & \hat{\beta_{0}}\bar{x} + \hat{\beta_{1}}\bar{x^{2}} = \bar{xy},
       \end{cases}
       \label{sys_fin}
	\end{equation}
	откуда МНК-оценку $\hat{\beta_{1}}$ наклона прямой регрессии находим по формуле Крамера
	\begin{equation}
	    \hat{\beta_{1}} = \frac{\bar{xy} - \bar{x} \cdot \bar{y}}{\bar{x^{2}} - (\bar{x})^{2}}
	    \label{beta_1_new}
	\end{equation}
	a МНК-оценку $\hat{\beta_{0}}$  определяем непосредственно из первого уравнения системы (\ref{sys_fin}):
	\begin{equation}
	    \hat{\beta_{0}} = \bar{y} - \bar{x}\hat{\beta_{1}}
	    \label{beta_0_new}
	\end{equation}
	Заметим, что определитель системы (\ref{sys_fin}):
	\begin{equation}
	    \bar{x^{2}} - (\bar{x})^{2} = \frac{1}{n}\sum_{}{}{(x_{i} - \bar{x})^{2}} = s_{x}^{2} > 0, 
	\end{equation}
	если среди значений $x_{1},...,x_{n}$ есть различные, что и будем предполагать.
	\newline
	Доказательство минимальности функции $Q(\beta_{0}, \beta_{1})$ в стационарной точке проведём с помощью известного достаточного признака экстремума функции двух переменных. Имеем:
	\begin{equation}
	    \frac{\partial ^{2} Q}{\partial \beta_{0}^{2}} = 2n, 
        \frac{\partial ^{2} Q}{\partial \beta_{1}^{2}} = 2\sum_{}{}{x_{i}^{2}} = 2n\bar{x^{2}}, 
        \frac{\partial ^{2} Q}{\partial \beta_{1} \partial \beta_{0}} = 2\sum_{}{}{x_{i}} = 2n\bar{x}
        \label{frac_eq}
	\end{equation}
	\begin{equation}
	    \bigtriangleup = \frac{\partial^{2}Q}{\partial \beta_{0}^{2}} \cdot \frac{\partial^{2}Q}{\partial \beta_{1}^{2}} - (\frac{\partial^{2}Q}{\partial \beta_{1} \partial \beta_{0}})^{2} = 
	    4n^{2}\bar{x^{2}} - 4n^2(\bar{x})^{2} = 
	    4n^{2}\left[\bar{x^{2}} - (\bar{x})^{2}\right] = 4n^{2}\left[ \frac{1}{n}\sum{}_{}{(x_{i} - \bar{x})}\right] = 4n^{2}s_{x}^{2} > 0.
	    \label{det_sys}
	\end{equation}
	Этот результат вместе с условием $\frac{\partial^{2}Q}{\partial \beta_{0}^{2}} = 2n > 0$ означает, что в стационарной точке функция Q имеет минимум.

    \subsection{Робастные оценки коэффициентов линейной регрессии}
    Робастность оценок коэффициентов линейной регрессии (т.е. их устойчивость по отношению к наличию в данных редких, но больших по величине выбросов) может быть обеспечена различными способами. Одним из них является использование метода наименьших модулей вместо метода наименьших квадратов:
    \begin{equation}
	    \sum_{i=1}^{n}{|y_{i} - \beta_{0} - \beta_{1}x_{i}|}\rightarrow \min_{\beta_{0}, \beta_{1}}
	    \label{min_abs}
	\end{equation}
    Напомним, что использование метода наименьших модулей в задаче оценивания параметра сдвига распределений приводит к оценке в виде выборочной медианы, обладающей робастными свойствами. В отличие от этого случая и от задач метода наименьших квадратов, на практике задача (\ref{min_abs}) решается численно. Соответствующие процедуры представлены в некоторых современных пакетах программ по статистическому анализу.
    \newline
    Здесь мы рассмотрим простейшую в вычистлительном отношении робастную альтернативу оценкам коэффициентов линейной регрессии по МНК. Для этого сначала запишем выражения для оценок (\ref{beta_0_new}) и (\ref{beta_1_new}) в другом виде: 
    \begin{equation}
        \hat{\beta_{1}} = \frac{\bar{xy} - \bar{x} \cdot \bar{y}}{\bar{x^{2}} - (\bar{x})^{2}} = \frac{k_{xy}}{s_{x}^{2}} = \frac{k_{xy}}{s_{x}s_{y}} \cdot \frac{s_{y}}{s_{x}} = r_{xy}\frac{s_{y}}{s_{x}},\\
        \hat{\beta_{0}} = \bar{y} - \bar{x}\hat{\beta_{1}}
        \label{new_coef_abs}
    \end{equation}
    В формулах (\ref{new_coef_abs}) заменим выборочные средние $\bar{x}$ и $\bar{y}$ соответственно на робастные выборочные медианы med x и med y, среднеквадратические отклонения $s_{x}$ и $s_{y}$ на робастные нормированные интерквартильные широты $q^{*}_{x}$ и $q^{*}_{y}$, выборочный коэффициент корреляции $r_{xy}$ — на знаковый коэффициент корреляции $r_{Q}$: 
    \begin{equation}
        \hat{\beta_{1}}_{R} = r_{Q}\frac{q^{*}_{y}}{q^{*}_{x}},
        \label{b_1R}
    \end{equation}
    \begin{equation}
        \hat{\beta_{0}}_{R} = med y - \hat{\beta_{1}}_{R} med x,
        \label{b_0R}
    \end{equation}
    \begin{equation}
        r_{Q} = \frac{1}{n}\sum_{i=1}^{n}{sgn(x_{i} - med x)sgn(y_{i} - med y)},
        \label{r_Q}
    \end{equation}
    \begin{multline}
        \\\\
        q^{*}_{y} = \frac{y_{(j)} -y_{(l)}}{k_{q}(n)}, 
        q^{*}_{x} = \frac{x_{(j)} - x_{(l)}}{k_{q}(n)}, \\ 
        \begin{cases}
             & [\frac{n}{4}] + 1 \text{ при } \frac{n}{4} \text{ дробном, } \\ 
             & \frac{n}{4} \text{ при } \frac{n}{4} \text{ целом. }
        \end{cases}\\
        j = n - l + 1\\
        sgn(z) = \begin{cases}
                    & 1 \text{ при } z > 0 \\ 
                    & 0 \text{ при } z = 0 \\
                    & -1 \text{ при } z < 0
                 \end{cases}\\
        \label{q*}        
    \end{multline}
    Уравнение регрессии здесь имеет вид 
    \begin{equation}
        y = \hat{\beta_{0}}_{R} +  \hat{\beta_{1}}_{R}x
        \label{y}
    \end{equation}
    Статистики выборочной медианы и интерквартильной широты обладают робастными свойствами в силу того, что основаны на центральных порядковых статистиках, малочувствительных к большим по величине выбросам в данных. Статистика выборочного знакового коэффициента корреляции робастна, так как знаковая функция sgn z чувствительна не к величине аргумента, а только к его знаку. Отсюда оценка прямой регрессии (\ref{y}) обладает очевидными робастными свойствами устойчивости к выбросам по координате y, но она довольно груба

 \subsection{Оценка качества}
Проводить оценку качества работы метод будем проводить при помощи метода наименьших квадратов \ref{min_abs}

\subsection{Метод максимального правдоподобия}
	Одним из универсальных методов оценивания является метод максимального правдоподобия, предложенный Р.Фишером (1921).
    Пусть $x_{1},...,x_{n}$ — случайная выборка из генеральной совокупности с плотностью вероятности $f(x,\theta$); $L(x_{1},... ,x_{n}, \theta)$ — функция правдоподобия (ФП), представляющая собой совместную плотность вероятности независимых с.в. $x_{1}, ... ,x_{n}$ и рассматриваемая как функция неизвестного параметра $\theta$:
    \begin{equation}
        L(x_{1},...,x_{n},\theta) = f(x_{1},\theta)f(x_{2},\theta)...f(x_{n}, \theta)
        \label{L()}
    \end{equation}
    \textit{Определение}. Оценкой максимального правдоподобия (о.м.п) будем называть такое значение $\hat{\theta_{мп}}$ из множества допустимых значений параметра $\theta$, для которого ФП принимает наибольшее значение при заданных $x_{1},...,x_{n}$:
    \begin{equation}
        \hat{\theta_{мп}} = \arg \max_{\theta}L(x_{1},...,x_{n},\theta)
        \label{theta_mp}
    \end{equation}
    Если ФП дважды дифференцируема, то её стационарные значения даются корнями уравнения
    \begin{equation}
        \frac{\partial L(x_{1},...,x_{n},\theta)}{\partial \theta} = 0
        \label{eq_min}
    \end{equation}
    Достаточным условием того, чтобы некоторое стационарное значениe $\Tilde{\theta}$ было локальным максимумом, является неравенство
    \begin{equation}
        \frac{\partial^{2}L}{\partial \theta^{2}}(x_{1},...,x_{n},\Tilde{\theta}) < 0
        \label{ineq_min}
    \end{equation}
    Определив точки локальных максимумов ФП (если их несколько), находят наибольший, который и даёт решение задачи (\ref{L()}).
    Часто проще искать максимум логарифма ФП, так как он имеет максимум в одной точке с ФП:
    \begin{equation}
        \frac{\partial \ln L}{\partial \theta}=\frac{1}{L}\frac{\partial L}{\partial \theta}, если L > 0
        \label{log_max}
    \end{equation}
    и соответственно решать уравнение
    \begin{equation}
         \frac{\partial \ln L}{\partial \theta}= 0
         \label{log_m=0}
    \end{equation}
    которое называют \textit{уравнением правдоподобия}.
    В задаче оценивания векторного параметра $\theta = (\theta_{1}, ... ,\theta_{m})$ аналогично (\ref{theta_mp}) находится максимум ФП нескольких аргументов: 
    \begin{equation}
        \hat{\theta_{мп}} = \arg \max_{\theta_{1}, \theta_{2}...\theta_{m}} L(x_{1}, x_{2},...x_{n}, \theta_{1}, \theta_{2},...\theta_{m})
        \label{multi_theta}
    \end{equation}
    и в случае дифференцируемости ФП выписывается система уравнений правдоподобия
    \begin{equation}
        \frac{\partial L}{\partial \theta_{k}} = 0 \text{  или  } \frac{\partial \ln L}{\partial \theta_{k}} = 0, k = 1,..m
    \end{equation}





    \subsection{Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}
    Исчерпывающей характеристикой изучаемой случайной величины является её закон распределения. Поэтому естественно стремление исследователей построить этот закон приближённо на основе статистических данных.
    \newline
    Сначала выдвигается гипотеза о виде закона распределения.
    \newline
    После того как выбран вид закона, возникает задача оценивания его параметров и проверки (тестирования) закона в целом.
    \newline
    Для проверки гипотезы о законе распределения применяются критерии согласия. Таких критериев существует много. Мы рассмотрим наиболее обоснованный и наиболее часто используемый в практике — критерий $\chi^{2}$ (хи-квадрат), введённый К.Пирсоном (1900 г.) для случая, когда параметры распределения известны. Этот критерий был существенно уточнён Р.Фишером (1924 г.), когда параметры распределения оцениваются по выборке, используемой для проверки.
    \newline
    Мы ограничимся рассмотрением случая одномерного распределения.
    \newline
    Итак, выдвинута гипотеза $H_{0}$ о генеральном законе распределения с функцией распределения F(x).
    \newline
    Рассматриваем случай, когда гипотетическая функция распределения F(x) не содержит неизвестных параметров.
    \newline
    Разобьём генеральную совокупность, т.е. множество значений изучаемой случайной величины X на k непересекающихся подмножеств $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$.
    \newline
    Пусть $p_{i} = P(X \in \Delta_{i}), i = 1, ... ,k$. 
    \newline
    Если генеральная совокупность — вся вещественная ось, то подмножества $\Delta_{i} = (a_{i-1},a_{i}]$ — полуоткрытые промежутки (i = 2, ... ,k$-$1). Крайние промежутки будут полубесконечными: $\Delta_{1} = (-\infty,a_{1}], \Delta_{k} = (a_{k-1},+\infty).$ В этом случае $p_{i} = F(a_{i})$$-$$F(a_{i-1}); a_{0} = -\infty, a_{k} = +\infty (i = 1, ... ,k).$
    \newline
    Отметим, что $\sum_{i=1}^{k}{p_{i}} = 1$.
    Будем предполагать, что все $p_{i}$ > 0 (i = 1, ... ,k).
    \newline
    Пусть, далее, $n_{1},n_{2}, ... ,n_{k}$ — частоты попадания выборочных элементов в подмножества $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$ соответственно.
    \newline
    В случае справедливости гипотезы $H_{0}$ относительные частоты $n_{i}/n$ при большом n должны быть близки к вероятностям $p_{i}$ (i = 1, ... ,k), поэтому за меру отклонения выборочного распределения от гипотетического с функцией F(x) естественно выбрать величину
    \begin{equation}
        Z = \sum_{i = 1}^{k}{c_{i}(\frac{n_{i}}{n} - p_{i})^{2}}, 
        \label{Z}
    \end{equation}
    где $c_{i}$ — какие-нибудь положительные числа (веса). К.Пирсоном в качестве весов выбраны числа $c_{i} = n/p_{i}$ (i = 1, ... ,k). Тогда получается статистика критерия хи-квадрат К.Пирсона
    \begin{equation}
        \chi^{2} = \sum_{i = 1}^{k}{\frac{n}{p_{i}}(\frac{n_{i}}{n} - p_{i})^{2}} = \sum_{i = 1}^{k}{\frac{(n_{i} - np_{i})^{2}}{np_{i}}}, 
        \label{chi_2}
    \end{equation}
    которая обозначена тем же символом, что и закон распределения хи-квадрат.
    \newline
    К.Пирсоном доказана теорема об асимптотическом поведении статистики $\chi^{2}$, указывающая путь её применения.
    \newline
    \textit{Теорема К.Пирсона}. Статистика критерия $\chi^{2}$ асимптотически распределена по закону $\chi^{2}$ с $k-1$ степенями свободы.
    \newline
    Это означает, что независимо от вида проверяемого распределения, т.е. функции F(x), выборочная функция распределения статистики $\chi^{2}$ при $n \rightarrow \infty$  стремится к функции распределения случайной величины с плотностью вероятности 
    \begin{equation}
        f_{k - 1}(x) = 
        \begin{cases}
            & 0 \text{ , } x  \leq 0  \\ 
            & \frac{1}{2^{\frac{k-1}{2}}\Gamma(\frac{k-1}{2})}x^{\frac{k-3}{2}}e^{-\frac{x}{2}}
            \text{ , } x>0 
        \end{cases}
        \label{f_k-1}
    \end{equation}
    Для прояснения сущности метода $\chi^{2}$ сделаем ряд замечаний.
    \newline
    \textit{Замечание 1}. Выбор подмножеств $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$ и их числа k в принципе ничем не регламентируется, так как $n \rightarrow \infty$. Но так как число n хотя и очень большое, но конечное, то k должно быть с ним согласовано. Обычно его берут таким же, как и для построения гистограммы, т.е. можно руководствоваться формулой
    \begin{equation}
        k \approx 1.72\sqrt[3]{n}
        \label{k_1}
    \end{equation}
    или формулой Старджесса
    \begin{equation}
        k \approx 1 + 3.3lgn
    \end{equation}
     При этом, если  $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$ — промежутки, то их длины удобно сделать равными, за исключением крайних — полубесконечных.
     \newline
    \textit{Замечание 2}. (о числе степеней свободы).
    Числом степеней свободы функции (по старой терминологии) называется число её независимых аргументов. Аргументами статистики $\chi^{2}$ являются частоты $n_{1},n_{2}, ... ,n_{k}$. Эти частоты связаны одним равенством $n_{1} + n_{2} + ... + n_{k}  = n$, а в остальном независимы в силу независимости элементов выборки. Таким образом, функция $\chi^{2}$  имеет $k-1$ независимых аргументов: число частот минус одна связь. В силу теоремы Пирсона число степеней свободы статистики $\chi^{2}$  отражается на виде асимптотической плотности $f_{k - 1}(x)$.
    \newline
    На основе общей схемы проверки статистических гипотез сформулируем следующее правило.
    \newline
    \textit{Правило проверки гипотезы о законе распределения по методу $\chi^{2}$}.
    \newline
    1. Выбираем уровень значимости $\alpha$.
     \newline
    2. По таблице [3, с. 358] находим квантиль $\chi^{2}_{1-\alpha}(k - 1)$ распределения хи-квадрат с k$-$1 степенями свободы порядка $1-\alpha$. 
    \newline
    3. С помощью гипотетической функции распределения F(x) вычисляем вероятности $p_{i} = P (X \in \Delta_{i})$, i = 1, ... ,k.
     \newline
    4. Находим частоты $n_{i}$ попадания элементов выборки в подмножества $\Delta_{i}$, i = 1, ... ,k. 
     \newline
    5. Вычисляем выборочное значение статистики критерия $\chi^{2}$:
    \begin{equation}
        \chi^{2}_{B} =\sum_{i = 1}^{k}{\frac{(n_{i} - np_{i})^{2}}{np_{i}}}.
        \label{chi_B}
    \end{equation}
    \newline
    6. Сравниваем $\chi^{2}_{B}$ и квантиль $\chi^{2}_{1-\alpha}(k-1)$.
    \newline
    а) Если $\chi^{2}_{B}$ < $\chi^{2}_{1-\alpha}$(k $-$ 1), то гипотеза $H_{0}$ на данном этапе проверки принимается. 
    \newline
    б) Если $\chi^{2}_{B} \geq \chi^{2}_{1-\alpha}(k -1)$, то гипотеза $H_{0}$ отвергается, выбирается одно из альтернативных распределений, и процедура проверки повторяется.
    \newline
    \textit{Замечание 3}. Из формулы (\ref{chi_2}) видим, что веса $c_{i} = n/p_{i}$ пропорциональны n, т.е. с ростом n увеличиваются. Отсюда следует, что если выдвинутая гипотеза неверна, то относительные частоты $n_{i}/n$ не будут близки к вероятностям $p_{i}$, и с ростом n величина  $\chi^{2}_{B}$  будет увеличиваться. При фиксированном уровне значимости$ \alpha $будет фиксировано пороговое число — квантиль $\chi^{2}_{1-\alpha}(k-1)$, поэтому, увеличивая n, мы придём к неравенству $\chi^{2}_{B}$ > $\chi^{2}_{1-\alpha}(k-1)$, т.е. с увеличением объёма выборки неверная гипотеза будет отвергнута.
    \newline
    Отсюда следует, что при сомнительной ситуации, когда $\chi^{2}_{B} \approx \chi^{2}_{1-\alpha}(k-1)$, можно попытаться увеличить объём выборки (например, в 2 раза), чтобы требуемое неравенство было более чётким.
    \newline
    \textit{Замечание 4}. Теория и практика применения критерия  $\chi^{2}$ указывают, что если для каких-либо подмножеств $\Delta_{i}$ (i = 1, ... ,k) условие $np_{i} \geq 5$ не выполняется, то следует объединить соседние подмножества (промежутки).
    \newline
    Это условие выдвигается требованием близости величин $\frac{(n_{i} -np_{i})}{\sqrt{np_{i}}}$, квадраты которых являются слагаемыми $\chi^{2}$  к нормальным N(0,1). Тогда случайная величина в формуле (\ref{chi_2}) будет распределена по закону, близкому к хи-квадрат. Такая близость обеспечивается достаточной численностью элементов в подмножествах $\Delta_{i}$ [1, с. 481-485].

\subsection{Доверительные интервалы для параметров нормального распределения}
	\subsubsection{Доверительный интервал для математического ожидания $m$ нормального распределения}
	Дана выборка ($x_{1},x_{2}, ... ,x_{n}$) объёма n из нормальной генеральной совокупности. На её основе строим выборочное среднее $\bar{x}$ и выборочное среднее квадратическое отклонение $s$. Параметры $m$ и $\sigma$ нормального распределения неизвестны.
	\newline
    Доказано, что случайная величина
    \begin{equation}
        T = \sqrt{n - 1}\frac{\bar{x} - m}{s}
        \label{T}
    \end{equation}
    называемая статистикой Стьюдента, распределена по закону Стьюдента с $n-1$ степенями свободы. Пусть $f_{T}(x)$ — плотность вероятности этого распределения. Тогда 
    \begin{multline}
        P\left(-x < \sqrt{n - 1}\frac{\bar{x} - m}{s} < x \right) = 
        P\left(-x < \sqrt{n - 1}\frac{m - \bar{x}}{s} < x \right) = \\\
        = \int_{-x}^{x}{f_{T}(t)dt} = 2 \int_{0}^{x}{f_{T}(t)dt} = 
        2\left(  \int_{-\infty}^{x}{f_{T}(t)dt} - \frac{1}{2} \right) = 2F_{T}(x) - 1
        \label{P_f_t}
    \end{multline}
    Здесь $F_{T}(x)$ — функция распределения Стьюдента с $n-1$ степенями свободы.
    \newline
    Полагаем $2F_{T}(x)-1 = 1-\alpha$, где $\alpha$ — выбранный уровень значимости. Тогда $F_{T}(x) = 1-\alpha/2$. Пусть $t_{1-\alpha/2}(n-1)$ - квантиль распределения Стьюдента с $n-1$ степенями свободы и порядка $1-\alpha/2$. Из предыдущих равенств мы получаем 
    \begin{equation}
             P\left(\bar{x} - \frac{sx}{\sqrt{n-1}} < m <  \bar{x} + \frac{sx}{\sqrt{n-1}}\right) = 2F_{T}(x) - 1 = 1 - \alpha,  \\
             P\left(\bar{x} - \frac{st_{1-\alpha/2}(n-1)}{\sqrt{n-1}} < m <  \bar{x} + \frac{st_{1-\alpha/2}(n-1)}{\sqrt{n-1}}\right)= 1 - \alpha,
        \label{P_m}         
    \end{equation}
    что и даёт доверительный интервал для $m$ с доверительной вероятностью $\gamma = 1-\alpha$ [1, с. 457-458].
    
    \subsubsection{Доверительный интервал для среднего квадратического отклонения $\sigma$ нормального распределения}
    Дана выборка ($x_{1},x_{2}, ... ,x_{n}$) объёма n из нормальной генеральной совокупности. На её основе строим выборочную дисперсию $s^{2}$. Параметры $m$ и $\sigma$ нормального распределения неизвестны. Доказано, что случайная величина $ns^{2}/\sigma^{2}$ распределена по закону $\chi^{2}$ с $n-1$ степенями свободы.
    \newline
    Задаёмся уровнем значимости $\alpha$ и находим квантили $\chi^{2}_{\alpha/2}(n-1)$ и $\chi^{2}_{1-\alpha/2}(n-1)$.
    \newline
    Это значит, что 
    \begin{equation}
        \begin{split}
        P\left(\chi^{2}(n-1) < \chi^{2}_{\alpha/2}(n-1)\right) = \alpha/2, \\
        P\left(\chi^{2}(n-1) < \chi^{2}_{1-\alpha/2}(n-1)\right) = 1-\alpha/2
        \end{split}
        \label{P_chi_2x2}        
    \end{equation}
    Тогда
    \begin{multline}
         P\left(\chi^{2}_{\alpha/2}(n-1) < \chi^{2}(n-1) < \chi^{2}_{1-\alpha/2}(n-1)\right) = \\\
          P\left(\chi^{2}(n-1) < \chi^{2}_{1-\alpha/2}(n-1)\right) -P\left(\chi^{2}(n-1) < \chi^{2}_{\alpha/2}(n-1)\right) = \\\ = 1 - \alpha/2 -\alpha/2 = 1 - \alpha
          \label{P_chi_2}
    \end{multline}
    Отсюда
    \begin{multline}
         P\left(\chi^{2}_{\alpha/2}(n-1) < \frac{ns^{2}}{\sigma^{2}} < \chi^{2}_{1-\alpha/2}(n-1)\right) =
          P\left(\frac{1}{\chi^{2}_{1-\alpha/2}(n-1)} < \frac{\sigma^{2}}{ns^{2}} < \frac{1}{\chi^{2}_{\alpha/2}(n-1)} \right) = \\\ =
          P\left(\frac{s\sqrt{n}}{\sqrt{\chi^{2}_{1-\alpha/2}(n-1)}} < \sigma <  \frac{s\sqrt{n}}{\sqrt{\chi^{2}_{\alpha/2}(n-1)}}\right) = 1- \alpha
          \label{interv}
    \end{multline}
    Окончательно
    \begin{equation}
         P\left(\frac{s\sqrt{n}}{\sqrt{\chi^{2}_{1-\alpha/2}(n-1)}} < \sigma <  \frac{s\sqrt{n}}{\sqrt{\chi^{2}_{\alpha/2}(n-1)}}\right) = 1- \alpha,
         \label{fin_interval}
    \end{equation}
    что и даёт доверительный интервал для $\sigma$ с доверительной вероятностью $\gamma = 1 - \alpha$ [1, с. 458-459].
    
    \subsection{Доверительные интервалы для математического ожидания $m$ и среднего квадратического отклонения $\sigma$ произвольного распределения при большом объёме выборки. Асимптотический подход}
    При большом объёме выборки для построения доверительных интервалов может быть использован асимптотический метод на основе центральной предельной теоремы.
    \subsubsection{Доверительный интервал для математического ожидания $m$ произвольной генеральной совокупности при большом объёме выборки}
    Выборочное среднее $\bar{x} = \frac{1}{n}\sum_{i = 1}^{n}{x_{i}}$ при большом объёме выборки является суммой большого числа взаимно независимых одинаково распределённых случайных величин. Предполагаем, что исследуемое генеральное распределение имеет конечные математическое ожидание $m$ и дисперсию $\sigma^{2}$. Тогда в силу центральной предельной теоремы центрированная и нормированная случайная величина $(\bar{x} - M\bar{x}) / \sqrt{D\bar{x}} = \sqrt{n}·(\bar{x}-m)/\sigma$ распределена приблизительно нормально с параметрами 0 и 1. Пусть
    \begin{equation}
        \Phi(x) = \frac{1}{2\pi}\int_{-\infty}^{x}{e^{-t^{2}/2}dt}
        \label{f_lapl}
    \end{equation}
    - функция Лапласа. Тогда
    \begin{multline}
        P\left(-x < \sqrt{n}\frac{\bar{x} - m}{\sigma} < x \right) = 
        P\left(-x < \sqrt{n}\frac{m - \bar{x}}{\sigma} < x \right) \approx \\\
        \approx \Phi(x) - \Phi(-x)=\Phi(x) - [1 - \Phi(x)] = 2\Phi(x) - 1
        \label{P_PHI}
    \end{multline}
    Отсюда
    \begin{equation}
        P\left(\bar{x} - \frac{\sigma x}{\sqrt{n}} < m < \bar{x} - \frac{\sigma x}{\sqrt{n}} \right) \approx 2\Phi(x) - 1
        \label{P_fin_PHI}
    \end{equation}
    Полагаем $2\Phi(x) - 1 = \gamma = 1 - \alpha$; тогда $\Phi(x) = 1 - \alpha/2$. Пусть $u_{1-\alpha/2}$ — квантиль нормального распределения N(0,1) порядка $1-\alpha/2$. Заменяя в равенстве (\ref{P_fin_PHI}) $\sigma$ на $s$, запишем его в виде
    \begin{equation}
        P\left(\bar{x} - \frac{su_{1-\alpha/2}}{\sqrt{n}} < m < \bar{x} - \frac{su_{1-\alpha/2}}{\sqrt{n}} \right) \approx \gamma,
        \label{P_fin_u}
    \end{equation}
    что и даёт доверительный интервал для $m$ с доверительной вероятностью $\gamma = 1-\alpha$ [1, с. 460].
    
\subsubsection{Доверительный интервал для среднего квадратического отклонения $\sigma$ произвольной генеральной совокупности при большом объёме выборки}

    Выборочная дисперсия $s^{2} = \sum_{i = 1}^{n}{\frac{(x_{i} - \bar{x})^{2}}{n}}$ при большом объёме выборки является суммой большого числа практически взаимно независимых случайных величин (имеется одна связь $\sum_{i=1}^{n}{x_{i}} = n\bar{x}$, которой при большом n можно пренебречь). Предполагаем, что исследуемая генеральная совокупность имеет конечные первые четыре момента.
    \newline
    В силу центральной предельной теоремы центрированная и нормированная случайная величина $(s^{2}-Ms^{2})/\sqrt{Ds^{2}}$ при большом объёме выборки n распределена приблизительно нормально с параметрами 0 и 1. Пусть $\Phi(x)$ — функция Лапласа (\ref{f_lapl}). Тогда
    \begin{equation}
        P\left(-x < \frac{s^{2}-Ms^{2}}{\sqrt{Ds^{2}}} < x\right)
        \approx \Phi(x) - \Phi(-x)=\Phi(x) - [1 - \Phi(x)] = 2\Phi(x) - 1
        \label{P_as_sigma}
    \end{equation}
Положим $2\Phi(x)-1 = \gamma = 1-\alpha$. Тогда $\Phi(x) = 1-\alpha/2$. Пусть $u_{1-\alpha/2}$ - корень этого уравнения - квантиль нормального распределения N(0,1) порядка $1-\alpha/2$. Известно, что $Ms^{2} = \sigma^{2} -\frac{\sigma^2}{n} \approx \sigma^{2}$ и $ Ds^{2} = \frac{\mu_{4} -\mu_{2}^{2}}{n} + o(\frac{1}{n}) \approx \frac{\mu_{4} -\mu_{2}^{2}}{n}$. Здесь $\mu_{k}$ - центральный момент k-го порядка генерального распределения; $\mu_2 = \sigma^2; \mu_4 = M[(x - Mx)^4]; o( \frac{1}{n} )$ - бесконечно малая высшего порядка, чем 1/n, при $n\rightarrow \infty$. Итак, $Ds^{2} \approx \frac{\mu_{4} -\mu_{2}^{2}}{n}$. Отсюда
    \begin{equation}
        Ds^{2} \approx \frac{\sigma^{4}}{n}(\frac{\mu_{4}}{\sigma^{4}} - 1) = 
        \frac{\sigma^{4}}{n}((\frac{\mu_{4}}{\sigma^{4}} - 3) + 2) = \frac{\sigma^{4}}{n}(E + 2) \approx \frac{\sigma^{4}}{n}(e + 2),
        \label{Ds_2}
    \end{equation}
    где E = $\frac{\mu_{4}}{\sigma^{4}} - 3$ — эксцесс генерального распределения, e = $\frac{m_{4}}{s^{4}} - 3$ — выборочный эксцесс; $m_{4} = \frac{1}{n}\sum_{i =1}^{n}{(x_{i} - \bar{x})^{4}}$  — четвёртый выборочный центральный момент. Далее,
    \begin{equation}
        \sqrt{Ds^{2}} \approx \frac{\sigma^{2}}{\sqrt{n}}\sqrt{e + 2}
        \label{sqrt_Ds}
    \end{equation}
    Преобразуем неравенства, стоящие под знаком вероятности в формуле
    \newline
    $P\left(-x < \frac{s^{2}-Ms^{2}}{\sqrt{Ds^{2}}} < x\right) = \gamma$:
    \begin{equation}
        \begin{split}
            -\sigma^{2}U < s^{2} -\sigma^{2} < \sigma^{2}U; \\
           \sigma^{2}(1-U) < s^{2} < \sigma^{2}(1 + U); \\
            1/[\sigma^{2}(1 + U)] < 1/s^{2} < 1/[\sigma^{2}(1-U)];\\
            s^{2}/(1 + U) < \sigma^{2} < s^{2}/(1-U);\\
            s(1 + U)^{-1/2} < \sigma < s(1-U)^{-1/2},
        \end{split}
        \label{multi_ineq}
    \end{equation}
    где $U = u_{1-\alpha/2} \sqrt{(e + 2)/n}$ или
    \newline
    $s(1 +  u_{1-\alpha/2}\sqrt{(e + 2)/n})^{-1/2} <\sigma < s(1-u_{1-\alpha/2}\sqrt{(e + 2)/n})^{-1/2}$.
    \newline
    Разлагая функции в биномиальный ряд и оставляя первые два члена, получим
    \begin{equation}
        s(1-0.5U) < \sigma < s(1 + 0.5U)
        \label{s_U}
    \end{equation}
     или
    \begin{equation}
        s(1-0.5u_{1-\alpha/2}\sqrt{(e + 2)/n}) < \sigma < s(1 + 0.5 u_{1-\alpha/2}\sqrt{(e + 2)/n})
        \label{s_u}
    \end{equation}
    Формулы (\ref{multi_ineq}) или (\ref{s_u}) дают доверительный интервал для $\sigma$ с доверительной вероятностью $\gamma = 1-\alpha$ [1, с. 461-462]. 
    \newline
    \textit{Замечание.} Вычисления по формуле (\ref{multi_ineq}) дают более надёжный результат, так как в ней меньше грубых приближений.


\section {Реализация}
Лабораторная работа выполнена с помощью встроенных средств языка программирования Python в среде разработки Visual Code. Исходный код лабораторной работы приведён в приложении.
 
\section{Результаты}

\subsection{Выборочные коэффициенты корреляции}

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c | c | c |}
\hline
 $\rho$= 0.1 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.09583 & 0.0881  & 0.0618  \\ \hline
 $E(z^2)$    & 0.06164 & 0.06158 & 0.05472 \\ \hline
 $D(z)$      & 0.05246 & 0.05382 & 0.0509  \\ \hline
&  & & \\  \hline
 $\rho$= 0.5 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.48972 & 0.46149 & 0.3333  \\ \hline
 $E(z^2)$    & 0.27144 & 0.24917 & 0.15597 \\ \hline
 $D(z)$      & 0.03161 & 0.0362  & 0.04488 \\ \hline
&  & & \\  \hline
 $\rho$= 0.9 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.89417 & 0.86592 & 0.7107  \\ \hline
 $E(z^2)$    & 0.80188 & 0.75436 & 0.53101 \\ \hline
 $D(z)$      & 0.00234 & 0.00454 & 0.02592 \\
\hline
\end{tabular}
 \caption{Двумерное нормальное распределение, n = 20}
\label{tab:norm_20}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c |}
\hline
 $\rho$= 0.1 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.09635 & 0.09191 & 0.0616  \\ \hline
 $E(z^2)$    & 0.0261  & 0.02536 & 0.02056 \\ \hline
 $D(z)$      & 0.01681 & 0.01692 & 0.01677 \\ \hline
&  & & \\  \hline
 $\rho$= 0.5 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.49818 & 0.47721 & 0.33587 \\ \hline
 $E(z^2)$    & 0.25849 & 0.23911 & 0.12774 \\ \hline
 $D(z)$      & 0.01031 & 0.01138 & 0.01493 \\ \hline
&  & & \\  \hline
 $\rho$= 0.9 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.89831 & 0.88219 & 0.7134  \\ \hline
 $E(z^2)$    & 0.80768 & 0.77943 & 0.51793 \\ \hline
 $D(z)$      & 0.00072 & 0.00118 & 0.00899 \\
\hline
\end{tabular}
 \caption{Двумерное нормальное распределение, n = 60}
\label{tab:norm_60}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c |}
\hline
 $\rho$= 0.1 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.09685 & 0.09097 & 0.0623  \\ \hline
 $E(z^2)$    & 0.01949 & 0.01842 & 0.01368 \\ \hline
 $D(z)$      & 0.01011 & 0.01015 & 0.0098  \\ \hline
&  & & \\  \hline
 $\rho$= 0.5 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.49763 & 0.47816 & 0.3355  \\ \hline
 $E(z^2)$    & 0.25309 & 0.23481 & 0.12132 \\ \hline
 $D(z)$      & 0.00546 & 0.00617 & 0.00876 \\ \hline
&  & & \\  \hline
 $\rho$= 0.9 & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$      & 0.89931 & 0.88653 & 0.71348 \\ \hline
 $E(z^2)$    & 0.80912 & 0.78653 & 0.5139  \\ \hline
 $D(z)$      & 0.00036 & 0.0006  & 0.00485 \\
\hline
\end{tabular}
 \caption{Двумерное нормальное распределение, n = 100}
\label{tab:norm_100}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c |}
\hline
 n= 20    & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$   & 0.7856  & 0.74974 & 0.589   \\ \hline
 $E(z^2)$ & 0.62594 & 0.57467 & 0.3803  \\ \hline
 $D(z)$   & 0.00877 & 0.01255 & 0.03338 \\ \hline
&  & & \\  \hline
 n= 60    & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$   & 0.7889  & 0.7685  & 0.58197 \\ \hline
 $E(z^2)$ & 0.62474 & 0.59391 & 0.34904 \\ \hline
 $D(z)$   & 0.00238 & 0.00332 & 0.01035 \\ \hline
&  & & \\  \hline
 n= 100   & r       & $r_S$   & $r_Q$   \\ \hline
 $E(z)$   & 0.79138 & 0.77284 & 0.58436 \\ \hline
 $E(z^2)$ & 0.62771 & 0.59925 & 0.348   \\ \hline
 $D(z)$   & 0.00143 & 0.00197 & 0.00652 \\
\hline
\end{tabular}
 \caption{Смесь нормальных распределений}
\label{tab:mix}
\end{table}

\subsection{Эллипсы рассеивания}

	\begin{figure}[H]
		    \centering
		    \includegraphics[width=1\textwidth]{n=3.png}
		    \caption{Двумерное нормальное распределение, n = 3}
		    \label{fig:f100}
	\end{figure}

	\begin{figure}[H]
	    \centering
	    \includegraphics[width=1\textwidth]{n=5.png}
	    \caption{Двумерное нормальное распределение, n = 5}
	    \label{fig:f100}
	\end{figure}
	
	\begin{figure}[H]
	    \centering
	    \includegraphics[width=1\textwidth]{n=20.png}
	    \caption{ Двумерное нормальное распределение, n = 20}
	    \label{fig:f20}
	\end{figure}
	
	\begin{figure}[H]
	    \centering
	    \includegraphics[width=1\textwidth]{n=60.png}
	    \caption{Двумерное нормальное распределение, n = 60}
	    \label{fig:f60}
	\end{figure}
	
	\begin{figure}[H]
	    \centering
	    \includegraphics[width=1\textwidth]{n=100.png}
	    \caption{Двумерное нормальное распределение, n = 100}
	    \label{fig:f100}
	\end{figure}

\subsection{Оценки коэффициентов линейной регрессии}
	\subsubsection{Выборка без возмущений}
	\begin{itemize}
	    \item{Критерий наименьших квадратов:}
	    $\hat{a}\approx 2.36$, $\hat{b}\approx 2.06,$ quality $ \approx  12.63 $
	    \item{Критерий наименьших модулей:}
	     $\hat{a}\approx$ 2.65, $\hat{b}\approx 2.14,$ quality $\approx  13.79$
	\end{itemize}
	\begin{figure}[H]
	    \centering
	    \includegraphics[width = 10cm, height = 8cm]{notperturbation_lab6.png}
	    \caption{Выборка без возмущений}
	    \label{w/o_dist}
	\end{figure}
	
	\subsubsection{Выборка с возмущениями}
	\begin{itemize}
	    \item{Критерий наименьших квадратов:}
	    $\hat{a}\approx 2.36$, $\hat{b}\approx 0.48,$ quality $\approx  38.23$
	    \item{Критерий наименьших модулей:}
	     $\hat{a}\approx 2.65$, $\hat{b}\approx 1.69,$ quality $\approx  30.76$
	\end{itemize}
	\begin{figure}[H]
	    \centering
	    \includegraphics[width = 10cm, height = 8cm]{perturbation_lab6.png}
	    \caption{Выборка с возмущениями}
	    \label{w/o_dist}
	\end{figure}


\subsection{Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}

	Метод максимального правдоподобия для нормального распределения:
	\newline
	$\hat{\mu} \approx -0.11, \hat{\sigma} \approx 1.05$
	
	\begin{table}[H]
	    \centering
	    \begin{tabular}{| c | c | c | c | c | c | c |}
    \hline
  $ i$ & Границы          &   $n_i$ &   $ p_i$ &   $np_i$ &  $ n_i - np_i$ &  $ \frac{(n_i - np_i)^2}{np_i} $ \\
\hline
   1 & ($-\infty$, -1.01] &    19 & 0.1562 &  15.62 &         3.38 &                          0.73 \\  \hline
   2 & [-1.01, -0.37]  &    19 & 0.2004 &  20.04 &        -1.04 &                          0.05 \\  \hline
   3 & [-0.37, 0.28]   &    26 & 0.2517 &  25.17 &         0.83 &                          0.03 \\  \hline
   4 & [0.28, 0.92]    &    17 & 0.2122 &  21.22 &        -4.22 &                          0.84 \\  \hline
   5 & [0.92, 1.56]    &    16 & 0.1201 &  12.01 &         3.99 &                          1.33 \\  \hline
   6 & [1.56, $\infty$)  &     3 & 0.0594 &   5.94 &        -2.94 &                          1.45 \\  \hline
  $ \Sigma $& -               &   100 & 1      & 100    &         0    & 4.43 = $\chi^{2}_B$ \\  \hline
\end{tabular}
	    \caption{ Вычисление $\chi^{2}_{B}$ при проверке гипотезы $H_{0}$ о нормальном законе распределения $N(x,\hat{\mu}, \hat{\sigma})$}
	    \label{tab:chi_2}
\end{table}
Количество промежутков k = 6.
    \\ \\
Уровень значимости $\alpha$= 0.05.
    \\ \\
    Тогда квантиль $\chi^{2}_{1-\alpha}(k-1)$ = $\chi^{2}_{0.95}(5)$. Из таблицы [3, с. 358] $\chi^{2}_{0.95}(5) \approx 11.07$. 
Сравнивая $\chi^{2}_{B} = 4.43$ и $\chi^{2}_{0.95}(5) \approx 11.07$, видим, что $\chi^{2}_{B}$ < $\chi^{2}_{0.95}(5)$.

	Метод максимального правдоподобия для распредления Лапласа:
	\newline
	$\hat{\mu} \approx -0.11, \hat{\sigma} \approx 1.05$
	
	\begin{table}[H]
	    \centering
	    \begin{tabular}{| c | c | c | c | c | c | c |}
    \hline
  $ i$ & Границы          &   $n_i$ &   $ p_i$ &   $np_i$ &  $ n_i - np_i$ &  $ \frac{(n_i - np_i)^2}{np_i} $ \\
\hline
   1 & ($-\infty$, -1.01] &     4 & 0.1562 &   3.91 &         0.09 &                          0    \\  \hline
   2 & [-1.01, -0.37]  &     3 & 0.2004 &   5.01 &        -2.01 &                          0.81 \\  \hline
   3 & [-0.37, 0.28]   &     6 & 0.2517 &   6.29 &        -0.29 &                          0.01 \\  \hline
   4 & [0.28, 0.92]    &     3 & 0.2122 &   5.31 &        -2.31 &                          1    \\  \hline
   5 & [0.92, 1.56]    &     5 & 0.1201 &   3    &         2    &                          1.33 \\  \hline
   6 & [1.56,$\infty$)   &     4 & 0.0594 &   1.48 &         2.52 &                          4.26 \\  \hline
  $ \Sigma $& -               &  25 & 1      &  25    &       0    &                    7.42 = $\chi^{2}_B$ \\  \hline
\end{tabular}
	    \caption{ Вычисление $\chi^{2}_{B}$ при проверке гипотезы $H_{0}$ о нормальном законе распределения $N(x,\hat{\mu}, \hat{\sigma})$}
	    \label{tab:Lchi_2}
\end{table}
Количество промежутков k = 6.
    \\ \\
Уровень значимости $\alpha$= 0.05.
    \\ \\
    Тогда квантиль $\chi^{2}_{1-\alpha}(k-1)$ = $\chi^{2}_{0.95}(5)$. Из таблицы [3, с. 358] $\chi^{2}_{0.95}(5) \approx 11.07$. 
Сравнивая $\chi^{2}_{B} = 4.43$ и $\chi^{2}_{0.95}(5) \approx 11.07$, видим, что $\chi^{2}_{B}$ < $\chi^{2}_{0.95}(5)$.

\subsection{Доверительные интервалы для параметров нормального распределения}
	\begin{table}[H]
	    \centering
	    \begin{tabular}{| c | c | c |}
\hline
 n = 20  & m                & $\sigma$               \\  \hline
         & -0.48 < m < 0.31 & 0.64 < $\sigma$ < 1.22 \\  \hline
         &                  &                        \\  \hline
 n = 100 & m                & $\sigma$               \\  \hline
         & -0.16 < m < 0.23 & 0.86 < $\sigma$ < 1.14 \\
\hline
	    \end{tabular}
	    \caption{Доверительные интервалы для параметров нормального распределения}
	    \label{tab:interv_simple}
	\end{table}
	
	\subsection{Доверительные интервалы для параметров произвольного распределения. Асимптотический подход}
		\begin{table}[H]
	    \centering
	    \begin{tabular}{| c | c | c |}
	    \hline
 n = 20  & m                & $\sigma$               \\  \hline
         & -0.37 < m < 0.58 & 1.00 < $\sigma$ < 1.21 \\  \hline
         &                  &                        \\  \hline
 n = 100 & m                & $\sigma$               \\  \hline
         & -0.17 < m < 0.23 & 0.96 < $\sigma$ < 1.08 \\  \hline
	    \end{tabular}
	    \caption{Доверительные интервалы для параметров произвольного распределения. Асимптотический подход}
	    \label{tab:interv_asimpt}
\end{table}

\section{Обсуждение}

\subsection{Выборочные коэффициенты корреляции и эллипсы рассеивания}
Для двумерного нормального распределения дисперсии выборочных коэффициентов корреляции упорядочены следующим образом: $r < r_{S} < r_{Q}$; для смеси распределений: $r_{Q} < r_{S} < r$.
\newline
Процент попавших элементов выборки в эллипс рассеивания (95$\%$-ная доверительная область) примерно равен его теоретическому значению (95$\%$) и увеличивается при уменьшении выборки и может достигать 100$\%$.

\subsection{Оценки коэффициентов линейной регрессии}

\begin{itemize}
\itemКритерий наименьших квадратов точнее оценивает коэффициенты линейной регрессии на выборке без возмущений. 
\item
Для выборки с возмущениями результат получается точнее при оценке критерием наименьших модулей.
\item
Таким образом, критерий наименьших модулей устойчив к редким выбросам, в отличие от критерия наименьших квадратов.
\end{itemize}

\subsection{ Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}

Заключаем, что гипотезы $H_{0}$ о нормальном законе распределения $N(x,\hat{\mu}, \hat{\sigma})$,  на уровне значимости $\alpha = 0.05$ согласуется с выборкой для нормального распределения $N(x, 0, 1)$ и с распределением Лапласа $L(x, 0, 1)$.

\subsection{Доверительные интервалы для параметров распределения}

\begin{itemize}
    \item Генеральные характеристики ($m$ = 0 и $\sigma$ = 1) накрываются построенными доверительными интервалами. 
    \item Также можно сделать вывод, что для большей выборки доверительные интервалы являются соответственно более точными, т.е. меньшими по длине.
    \item Доверительные интервалы для параметров нормального распределения более надёжны, так как основаны на точном, а не асимптотическом распределении.
\end{itemize}

\section{Приложения}
Репозиторий на GitHub с релизацией: \href{https://github.com/WiillyWonka/MatStat}{github.com}.
\end{document}
